Facebook (briefly) banned one of India’s largest pages – for warning people about Fascism

On Sunday, social media giant Facebook banned one of India’s most active pages, run by Dhruv Rathee a popular YouTuber with more than 1.7 million subscribers.

Rathee, a self-described “political centrist”, is often critical of the Modi government and has, as a result, attracted criticism from supporters of the ruling Bharatiya Janata Party.

Ironically, the post he was banned for was pointing to the dangers of fascism and outlining the process by which the Nazi party and Adolf Hitler rose to power in Germany. Rathee’s post implicitly drew parallels to present-day India.

The incident is one in a trend that points to the enabling role played by social media giants in the global rise of the far right.

Against community standards?

Early on Monday, Rathee took to Twitter to complain that he was banned from posting on his Facebook page. The reason for the ban was a post on Hitler which, said Facebook, went against its community standards.

Rathee, however, pointed out that his post contained nothing abusive but sought to educate people about the rise of Adolf Hitler using information available in the Encyclopaedia Britannica. “I simply underlined some portions from the article pointing out that Hitler used propaganda, was more popular than the Nazi party, received support from industrialists and never married in order to further his career,” Rathee told Scroll.in. “I wanted to point out the dangers of the rise of Hitler and the parallels in India. How can that be against the standards of Facebook?”

That this was criticism, not endorsement, of Fascism was clear to the users of the page as well. “Adolf Modi Or Narendra Hitler?” asked one commenter rhetorically, while another said, “striking similarity”. Facebook itself states that it looks at comments in order to decide the context of a post.

The ban led to a sharp reaction from prominent public figures early on Monday. As a result, Facebook revoked its action within around 12 hours. “In this case, our content reviewers mistakenly removed a piece of content posted by Dhruv Rathee, leading to the temporary disabling of his account,” a Facebook spokesperson told Scroll.in over email. “We were alerted to our error, and acted swiftly to restore Dhruv’s profile and content. We’re really sorry for any inconvenience caused.”

Social media and the far right



While in this case, Facebook’s response was quick, the incident points to a growing concern that social media giants such as Facebook and Twitter are playing an enabling role for the rise of the far right.

Across the globe – Donald Trump in the United States, Jair Bolsarano in Brazil, Recep Erdogan in Turkey, to name a few – far-right leaders have used social media as a crucial propaganda tool to rise to power. Social media platforms have allowed like-minded people on the far right to organise, making acceptable thoughts and ideologies that were seen as beyond the pale in a more analogue world.

In January, the United States-based news website Vox mined academic research to point out that social media websites “overall give far-right parties and authoritarians an advantage” by “helping them stoke social divisions”. Given the nature of the medium, which rewards anger, ideologies that sought to heal social divisions did not achieve as much success on social media.

Content moderation

To add to this institutional bias are reports which show that content moderation processes on Facebook – the largest social media website in the world – have significant gaps.

For one, the size of Facebook’s content-moderation team is too small to adequately manage the incredible numbers that the website attracts. More than 2.3 billion people use Facebook every month, making it larger than Christianity, the most populous religion globally. Moreover, Facebook users often use the website to broadcast their lives in incredible detail: more than a billion photos are posted on the website every day.

To police this ocean of content, Facebook has only about 15,000 content reviewers (as per latest figures released by Facebook in December, 2018). Facebook also uses automated technologies in order to make up for these small numbers. However, the strategy has had mixed results.

On Friday, after a terror attack on two mosques in New Zealand was livestreamed on the Internet, Facebook scrambled to stop the upload of the gruesome video onto its website using a mixture of automated tools and human content moderation. While it was able to block a majority of the videos at the point of upload, even then it faced a 20% failure rate. And given just how big the website is, this proportion represented a very large absolute number: about 300,000 videos of men, women and children being shot in Christchurch were still posted onto Facebook.

While the ban on Rathee’s page was soon revoked, that is perhaps unremarkable, given his popularity and the instant outrage it generated. What is more worrying is how Facebook’s content moderation flagged a post that was trying to warn Indians of the dangers of Fascism. Not every Facebook user would be able to attract the attention Rathee got. The solution is to fix Facebook’s content moderation so that the website encourages voices speaking against the far right, instead of suppressing them.

Also read:

Are WhatsApp, Facebook, and Twitter ready for the Indian election?

Meet Dhruv Rathee, the 23-year-old YouTuber whose videos are infuriating Modi’s admirers

The Indian YouTube wars: Political video influencers are heating up the internet in election year
